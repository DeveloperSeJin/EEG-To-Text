corpus_bleu_score = [0.14227520026246002, 0.016300010770635337, 0.0019619928976265097, 1.1385685438427776e-79]
sacre_blue_score = {'score': 0.0903094408278671, 'counts': [8823, 130, 13, 0], 'totals': [59979, 58581, 57183, 55785], 'precisions': [14.710148551993198, 0.2219149553609532, 0.022734029344385568, 0.0008962982880702698], 'bp': 1.0, 'sys_len': 59979, 'ref_len': 32886}
rouge_scores = {'rouge-1': {'r': 0.20231892683950048, 'p': 0.09790861971097073, 'f': 0.12742677344362183}, 'rouge-2': {'r': 0.0011323283598680069, 'p': 0.000707613724329333, 'f': 0.000846673980647998}, 'rouge-l': {'r': 0.15158228925574832, 'p': 0.07127782704137206, 'f': 0.093665655690058}}
wer_scores = 1.8575720723943057
cer_scores = 1.2069489736909567
corpus_bleu_score_with_tf = [0, 0, 0, 0]
sacre_blue_score_with_tf = {'score': 0.0, 'counts': [0, 0, 0, 0], 'totals': [0, 0, 0, 0], 'precisions': [0.0, 0.0, 0.0, 0.0], 'bp': 0.0, 'sys_len': 0, 'ref_len': 32886}
rouge_scores_with_tf = Hypothesis is empty
wer_scores_with_tf = 1.0
cer_scores_with_tf = 1.0
