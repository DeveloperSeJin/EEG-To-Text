corpus_bleu_score = [0.1384335656629659, 0.016498509973926864, 0.0019831269912420645, 1.1477544942140995e-79]
sacre_blue_score = {'score': 0.07449117645379874, 'counts': [9433, 160, 6, 0], 'totals': [64042, 62644, 61246, 59848], 'precisions': [14.729396333656037, 0.2554115318306622, 0.009796558142572576, 0.000835449806175645], 'bp': 1.0, 'sys_len': 64042, 'ref_len': 32886}
rouge_scores = {'rouge-1': {'r': 0.23166519052451684, 'p': 0.10288106779206624, 'f': 0.1379488530925589}, 'rouge-2': {'r': 0.0028193798708440894, 'p': 0.0014465038369923962, 'f': 0.0018457796864405998}, 'rouge-l': {'r': 0.1602764878276855, 'p': 0.06864237332399646, 'f': 0.0930581246986328}}
wer_scores = 2.0313327133557477
cer_scores = 1.2146455463016508
corpus_bleu_score_with_tf = [0, 0, 0, 0]
sacre_blue_score_with_tf = {'score': 0.0, 'counts': [0, 0, 0, 0], 'totals': [0, 0, 0, 0], 'precisions': [0.0, 0.0, 0.0, 0.0], 'bp': 0.0, 'sys_len': 0, 'ref_len': 32886}
rouge_scores_with_tf = Hypothesis is empty
wer_scores_with_tf = 1.0
cer_scores_with_tf = 1.0
